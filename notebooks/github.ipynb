{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github Experiments\n",
    "This notebook sets up a local environment for simulating queries on user data. It imports the Github dataset from https://www.gharchive.org/ and launches a `tiresias` client for each user in the dataset, allowing us to run queries on the full dataset and simulate a distributed setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "Let's download and process the github dataset. We'll parse out the common event types and load them into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip, json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Github dataset\n",
    "path = lambda x: os.path.join(\"/tmp/\", x)\n",
    "if os.path.exists(path(\"github.json.gz\")):\n",
    "    os.remove(path(\"github.json.gz\"))\n",
    "url = \"https://data.gharchive.org/2015-01-01-1.json.gz\"\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open(path(\"github.json.gz\"), 'ab') as fout:\n",
    "    fout.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the export tables\n",
    "tables = {}\n",
    "def export():\n",
    "    for table_name, rows in tables.items():\n",
    "        pd.DataFrame(rows).to_csv(path(\"%s.csv\" % table_name), index=False)\n",
    "\n",
    "def CreateEvent(obj):\n",
    "    if \"create_event\" not in tables:\n",
    "        tables[\"create_event\"] = []\n",
    "    tables[\"create_event\"].append({\n",
    "        \"id\": obj[\"id\"],\n",
    "        \"user_id\": obj[\"actor\"][\"login\"],\n",
    "        \"repo_name\": obj[\"repo\"][\"name\"],\n",
    "        \"timestamp\": obj[\"created_at\"],\n",
    "        \"master\": obj[\"payload\"][\"master_branch\"],\n",
    "        \"description\": obj[\"payload\"][\"description\"],\n",
    "        \"ref\": obj[\"payload\"][\"ref\"],\n",
    "        \"ref_type\": obj[\"payload\"][\"ref_type\"],\n",
    "    })\n",
    "\n",
    "def PushEvent(obj):\n",
    "    if \"push_event\" not in tables:\n",
    "        tables[\"push_event\"] = []\n",
    "    tables[\"push_event\"].append({\n",
    "        \"id\": obj[\"id\"],\n",
    "        \"user_id\": obj[\"actor\"][\"login\"],\n",
    "        \"repo_name\": obj[\"repo\"][\"name\"],\n",
    "        \"timestamp\": obj[\"created_at\"],\n",
    "        \"branch\": obj[\"payload\"][\"ref\"],\n",
    "        \"nb_commits\": obj[\"payload\"][\"size\"],\n",
    "    })\n",
    "\n",
    "def WatchEvent(obj):\n",
    "    if \"watch_event\" not in tables:\n",
    "        tables[\"watch_event\"] = []\n",
    "    tables[\"watch_event\"].append({\n",
    "        \"id\": obj[\"id\"],\n",
    "        \"user_id\": obj[\"actor\"][\"login\"],\n",
    "        \"repo_name\": obj[\"repo\"][\"name\"],\n",
    "        \"timestamp\": obj[\"created_at\"],\n",
    "    })\n",
    "\n",
    "def ReleaseEvent(obj):\n",
    "    if \"release_event\" not in tables:\n",
    "        tables[\"release_event\"] = []\n",
    "    tables[\"release_event\"].append({\n",
    "        \"id\": obj[\"id\"],\n",
    "        \"user_id\": obj[\"actor\"][\"login\"],\n",
    "        \"repo_name\": obj[\"repo\"][\"name\"],\n",
    "        \"timestamp\": obj[\"created_at\"],\n",
    "        \"action\": obj[\"payload\"][\"action\"],\n",
    "        \"tag_name\": obj[\"payload\"][\"release\"][\"tag_name\"],\n",
    "    })\n",
    "\n",
    "def PullRequestEvent(obj):\n",
    "    if \"pull_request_event\" not in tables:\n",
    "        tables[\"pull_request_event\"] = []\n",
    "    tables[\"pull_request_event\"].append({\n",
    "        \"id\": obj[\"id\"],\n",
    "        \"user_id\": obj[\"actor\"][\"login\"],\n",
    "        \"repo_name\": obj[\"repo\"][\"name\"],\n",
    "        \"timestamp\": obj[\"created_at\"],\n",
    "        \"action\": obj[\"payload\"][\"action\"],\n",
    "        \"body\": obj[\"payload\"][\"pull_request\"][\"body\"],\n",
    "        \"state\": obj[\"payload\"][\"pull_request\"][\"state\"],\n",
    "        \"commits\": obj[\"payload\"][\"pull_request\"][\"commits\"],\n",
    "        \"additions\": obj[\"payload\"][\"pull_request\"][\"additions\"],\n",
    "        \"deletions\": obj[\"payload\"][\"pull_request\"][\"deletions\"],\n",
    "        \"changed_files\": obj[\"payload\"][\"pull_request\"][\"changed_files\"]\n",
    "    })\n",
    "\n",
    "def IssuesEvent(obj):\n",
    "    if \"issues_event\" not in tables:\n",
    "        tables[\"issues_event\"] = []\n",
    "    tables[\"issues_event\"].append({\n",
    "        \"id\": obj[\"id\"],\n",
    "        \"user_id\": obj[\"actor\"][\"login\"],\n",
    "        \"repo_name\": obj[\"repo\"][\"name\"],\n",
    "        \"timestamp\": obj[\"created_at\"],\n",
    "        \"action\": obj[\"payload\"][\"action\"],\n",
    "        \"title\": obj[\"payload\"][\"issue\"][\"title\"],\n",
    "        \"body\": obj[\"payload\"][\"issue\"][\"body\"],\n",
    "        \"comments\": obj[\"payload\"][\"issue\"][\"comments\"],\n",
    "        \"state\": obj[\"payload\"][\"issue\"][\"state\"],\n",
    "    })\n",
    "    \n",
    "def ForkEvent(obj):\n",
    "    if \"fork_event\" not in tables:\n",
    "        tables[\"fork_event\"] = []\n",
    "    tables[\"fork_event\"].append({\n",
    "        \"id\": obj[\"id\"],\n",
    "        \"user_id\": obj[\"actor\"][\"login\"],\n",
    "        \"repo_name\": obj[\"repo\"][\"name\"],\n",
    "        \"timestamp\": obj[\"created_at\"],\n",
    "        \"forkee_name\": obj[\"payload\"][\"forkee\"][\"full_name\"],\n",
    "        \"forkee_description\": obj[\"payload\"][\"forkee\"][\"description\"],\n",
    "        \"forkee_size\": obj[\"payload\"][\"forkee\"][\"size\"],\n",
    "        \"forkee_stargazers\": obj[\"payload\"][\"forkee\"][\"stargazers_count\"],\n",
    "        \"forkee_watchers\": obj[\"payload\"][\"forkee\"][\"watchers_count\"],\n",
    "        \"forkee_has_issues\": obj[\"payload\"][\"forkee\"][\"has_issues\"],\n",
    "        \"forkee_has_downloads\": obj[\"payload\"][\"forkee\"][\"has_downloads\"],\n",
    "        \"forkee_has_wiki\": obj[\"payload\"][\"forkee\"][\"has_wiki\"],\n",
    "        \"forkee_has_pages\": obj[\"payload\"][\"forkee\"][\"has_pages\"],\n",
    "    })\n",
    "        \n",
    "def DeleteEvent(obj):\n",
    "    if \"delete_event\" not in tables:\n",
    "        tables[\"delete_event\"] = []\n",
    "    tables[\"delete_event\"].append({\n",
    "        \"id\": obj[\"id\"],\n",
    "        \"user_id\": obj[\"actor\"][\"login\"],\n",
    "        \"repo_name\": obj[\"repo\"][\"name\"],\n",
    "        \"ref\": obj[\"payload\"][\"ref\"],\n",
    "        \"ref_type\": obj[\"payload\"][\"ref_type\"],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table push_event has 4150 rows.\n",
      "Table pull_request_event has 293 rows.\n",
      "Table watch_event has 649 rows.\n",
      "Table issues_event has 330 rows.\n",
      "Table create_event has 727 rows.\n",
      "Table fork_event has 222 rows.\n",
      "Table delete_event has 81 rows.\n",
      "Table release_event has 43 rows.\n",
      "Identified 3100 unique users.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the dataset and generate the tables\n",
    "with gzip.open(path(\"github.json.gz\"), \"rt\") as fin:\n",
    "    for event in map(json.loads, fin):\n",
    "        if event[\"type\"] == \"CreateEvent\":        CreateEvent(event)\n",
    "        elif event[\"type\"] == \"PushEvent\":        PushEvent(event)\n",
    "        elif event[\"type\"] == \"WatchEvent\":       WatchEvent(event)\n",
    "        elif event[\"type\"] == \"ReleaseEvent\":     ReleaseEvent(event)\n",
    "        elif event[\"type\"] == \"PullRequestEvent\": PullRequestEvent(event)\n",
    "        elif event[\"type\"] == \"IssuesEvent\":      IssuesEvent(event)\n",
    "        elif event[\"type\"] == \"ForkEvent\":        ForkEvent(event)\n",
    "        elif event[\"type\"] == \"DeleteEvent\":      DeleteEvent(event)\n",
    "        else: pass\n",
    "\n",
    "# Convert the tables into dataframes\n",
    "userids = set()\n",
    "dataframes = {}\n",
    "for table_name, rows in tables.items():\n",
    "    df = pd.DataFrame(rows)\n",
    "    dataframes[table_name] = df\n",
    "    userids.update(df[\"user_id\"].values.tolist())\n",
    "    print(\"Table %s has %s rows.\" % (table_name, len(df)))\n",
    "print(\"Identified %s unique users.\" % len(userids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiresias Server\n",
    "Let's set up the tiresias server. We'll launch it in the background and tell it to listen on port 3000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "server = subprocess.Popen([\"tiresias-server\", \"--port\", \"3000\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiresias Clients\n",
    "Now let's launch a client for each user in the dataset, configure the github app schema, and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 28.22it/s]\n",
      "10it [00:01,  7.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from json import dumps, loads\n",
    "\n",
    "schema = {}\n",
    "for table_name, df in dataframes.items():\n",
    "    schema[table_name] = {\n",
    "        \"description\": \"\",\n",
    "        \"columns\": {c: {\"type\": \"float\", \"description\": \"\"} for c in df.columns}\n",
    "    }\n",
    "    \n",
    "userids = list(userids)[:100] # Let's just use the first few users\n",
    "\n",
    "clients = []\n",
    "for i, userid in tqdm(enumerate(userids)):\n",
    "    port = 8000 + i\n",
    "    client = subprocess.Popen([\n",
    "        \"tiresias\", \n",
    "        \"--db_port\", str(port), \n",
    "        \"--db_dir\", path(\"tiresias/%s\" % port)\n",
    "    ])\n",
    "    clients.append(client)\n",
    "sleep(10) # Wait a few secs for them to launch\n",
    "\n",
    "for i, userid in tqdm(enumerate(userids)):\n",
    "    port = 8000 + i\n",
    "    payload = {}\n",
    "    for table_name, df in dataframes.items():\n",
    "        payload[table_name] = df[df[\"user_id\"] == userid].to_dict('records')\n",
    "    requests.get(\"http://localhost:%s/app/github/register\" % port, params={\"schema\": dumps(schema)})\n",
    "    requests.get(\"http://localhost:%s/app/github/insert\" % port, params={\"payload\": dumps(payload)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://127.0.0.1:3000/query\", params={\n",
    "    \"query\": dumps({\n",
    "        \"type\": \"basic\",\n",
    "        \"epsilon\": 10.0,\n",
    "        \"featurizer\": \"SELECT 1.0*COUNT(*) FROM github.create_event\",\n",
    "        \"aggregator\": \"mean\",\n",
    "    })\n",
    "})\n",
    "qid = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'basic',\n",
       " 'epsilon': 10.0,\n",
       " 'featurizer': 'SELECT 1.0*COUNT(*) FROM github.create_event',\n",
       " 'aggregator': 'mean',\n",
       " 'id': '031499a8-8803-4f3f-93df-76d99968de88',\n",
       " 'status': 'COMPLETE',\n",
       " 'count': 10,\n",
       " 'result': 1.999997929993876}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loads(requests.get(\"http://127.0.0.1:3000/query/%s\" % qid).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Let's kill all the processes we launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for client in clients:\n",
    "    client.kill()\n",
    "server.kill()\n",
    "shutil.rmtree(path(\"tiresias\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
